{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project 4-RNN-Text-Generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TimothyJan/RNN_Text_Generation/blob/main/RNN_Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spring 2022 - CPSC 585-section-13883\n",
        "Project 4 - Recurrent Neural Networks Text Generation\n",
        "Sean Javiya\n",
        "Timothy Jan\n",
        "Timothy Kheang\n",
        "\n",
        "In Project 4, we use an RNN-based language model to generate text for a creative application.\n",
        "\n",
        "Goals for this project are:\n",
        "<ul>\n",
        "  <li>Reading about how RNNs can be used to generate text, and examining several different applications after training on different text corpora.</li>\n",
        "  <li>Using a multi-layer RNN to train and sample from a character-level language model.</li>\n",
        "  <li>Adapting and reusing published model code.</li>\n",
        "  <li>Having some fun.</li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "0JqoLNUQL6oU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCr_lbTxLs6e"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import urllib.request, json "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We started with the Tensorflow tutorial <a href=\"https://www.tensorflow.org/text/tutorials/text_generation\">Text generation with an RNN</a> and experimented with different datasets such as the Elder Scrolls, Aesop Fables, Harry Potter and the original Shakespeare dataset as used in the tutorial."
      ],
      "metadata": {
        "id": "S4ctoJPB_paf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#'''imported the elder scrolls json file'''\n",
        "#with urllib.request.urlopen(\"https://raw.githubusercontent.com/hmi-utwente/video-game-text-corpora/master/The%20Elder%20Scrolls/data/imperial_library_20200626.json\") as url:\n",
        "#    data_dict = json.loads(url.read().decode())\n",
        "#text=\"\"\n",
        "#for key in data_dict:\n",
        "#  text += data_dict[key]['description'] +' ' + data_dict[key]['title'] + ' ' + data_dict[key]['text']\n",
        "#text[:100]\n",
        "\n",
        "#'''imported the aesops json file'''\n",
        "#with urllib.request.urlopen(\"https://raw.githubusercontent.com/itayniv/aesop-fables-stories/master/public/aesopFables.json\") as url:\n",
        "#    data_dict = json.loads(url.read().decode())\n",
        "#text=\"\"\n",
        "#for key, value in data_dict.items():\n",
        "#  for story in value:\n",
        "#    temp = dict(story)\n",
        "#    text += str(temp[\"story\"]) + \" \"\n",
        "#print(text[:100])\n",
        "\n",
        "# '''imported the Harry Potter text files'''\n",
        "text=\"\"\n",
        "with urllib.request.urlopen(\"https://raw.githubusercontent.com/ErikaJacobs/Harry-Potter-Text-Mining/master/Book%20Text/HPBook1.txt\") as url:\n",
        "    text += url.read().decode()\n",
        "with urllib.request.urlopen(\"https://raw.githubusercontent.com/ErikaJacobs/Harry-Potter-Text-Mining/master/Book%20Text/HPBook2.txt\") as url:\n",
        "    text += url.read().decode()\n",
        "with urllib.request.urlopen(\"https://raw.githubusercontent.com/ErikaJacobs/Harry-Potter-Text-Mining/master/Book%20Text/HPBook3.txt\") as url:\n",
        "    text += url.read().decode()\n",
        "with urllib.request.urlopen(\"https://raw.githubusercontent.com/ErikaJacobs/Harry-Potter-Text-Mining/master/Book%20Text/HPBook4.txt\") as url:\n",
        "    text += url.read().decode()\n",
        "with urllib.request.urlopen(\"https://raw.githubusercontent.com/ErikaJacobs/Harry-Potter-Text-Mining/master/Book%20Text/HPBook5.txt\") as url:\n",
        "    text += url.read().decode()\n",
        "with urllib.request.urlopen(\"https://raw.githubusercontent.com/ErikaJacobs/Harry-Potter-Text-Mining/master/Book%20Text/HPBook6.txt\") as url:\n",
        "    text += url.read().decode()\n",
        "with urllib.request.urlopen(\"https://raw.githubusercontent.com/ErikaJacobs/Harry-Potter-Text-Mining/master/Book%20Text/HPBook7.txt\") as url:\n",
        "    text += url.read().decode()\n",
        "\n",
        "# '''Shakespearen dataset'''\n",
        "# path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "# # Read, then decode for py2 compat.\n",
        "# text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# # length of text is the number of characters in it\n",
        "# print(f'Length of text: {len(text)} characters')\n",
        "# print(text[:100])\n",
        "\n",
        "# unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "id": "PjHAQwPv2ZQj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c310f88-8c79-4997-eb3d-d8eb16810686"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "104 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Process the text</h1>\n",
        "<h2>Vectorize the text.</h2>\n",
        "Before training, we need to convert the strings to a numerical representation.\n",
        "The tf.keras.layers.StringLookup layer can convert each character into a numeric ID. It just needs the text to be split into tokens first."
      ],
      "metadata": {
        "id": "4c9STpH-_1qO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "id": "b6zWduFuuaIs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fd4d641-e42d-4bef-d9b0-9ce5b4f0611c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the tf.keras.layers.StringLookup layer. It converts from tokens to character IDs:"
      ],
      "metadata": {
        "id": "wwsioWOgu7Pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)\n",
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "id": "ztbeaqT886Ni",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "127700f4-f8ff-4b71-85a5-601ca01a6e16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[66, 67, 68, 69, 70, 71, 72], [89, 90, 91]]>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This layer recovers the characters from the vectors of IDs, and returns them as a tf.RaggedTensor of characters:"
      ],
      "metadata": {
        "id": "YMRHxp5gvSsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "id": "fHnRKiJZuEN_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09ef9749-f9a9-4ba3-d6fd-8dd176fa50b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use tf.strings.reduce_join to join the characters back into strings."
      ],
      "metadata": {
        "id": "fQsbjGoQvlZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "wU3pOFakvUMx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5a0489e-9271-481d-ee0f-697c562ccd3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n"
      ],
      "metadata": {
        "id": "jQJmm671vqst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>The Prediction Task</h2>\n",
        "Given a character, or a sequence of characters, what is the most probable next character? This is the task we train the model to perform. The input to the model will be a sequence of characters, and we train the model to predict the outputâ€”the following character at each time step.\n",
        "\n",
        "Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?\n",
        "\n",
        "<h2>Create training examples and targets</h2>\n",
        "Next we divide the text into example sequences. Each input sequence will contain seq_length characters from the text.\n",
        "\n",
        "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
        "\n",
        "So we break the text into chunks of seq_length+1. For example, say seq_length is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
        "\n",
        "To do this we first use the tf.data.Dataset.from_tensor_slices function to convert the text vector into a stream of character indices."
      ],
      "metadata": {
        "id": "ygvRQ68jwFGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "id": "n07gqUwPvtCD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b36376fd-a89e-4f14-f596-2641aa28ed45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(6325064,), dtype=int64, numpy=array([ 5, 53, 70, ..., 33, 24,  2])>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "id": "iWdmxTW4wUQ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe4dcf6c-2418-47f8-952e-08061f2365f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"\n",
            "T\n",
            "e\n",
            "x\n",
            "t\n",
            "\"\n",
            "@\n",
            "\"\n",
            "C\n",
            "h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The batch method lets us easily convert these individual characters to sequences of the desired size."
      ],
      "metadata": {
        "id": "kqaTSg_kwsg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "id": "zsYC0q3OwhPk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8660df98-9c3c-43fc-f1d4-b62c548fff90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'\"' b'T' b'e' b'x' b't' b'\"' b'@' b'\"' b'C' b'h' b'a' b'p' b't' b'e'\n",
            " b'r' b'\"' b'@' b'\"' b'B' b'o' b'o' b'k' b'\"' b'\\n' b'\"' b'T' b'H' b'E'\n",
            " b' ' b'B' b'O' b'Y' b' ' b'W' b'H' b'O' b' ' b'L' b'I' b'V' b'E' b'D'\n",
            " b' ' b' ' b'M' b'r' b'.' b' ' b'a' b'n' b'd' b' ' b'M' b'r' b's' b'.'\n",
            " b' ' b'D' b'u' b'r' b's' b'l' b'e' b'y' b',' b' ' b'o' b'f' b' ' b'n'\n",
            " b'u' b'm' b'b' b'e' b'r' b' ' b'f' b'o' b'u' b'r' b',' b' ' b'P' b'r'\n",
            " b'i' b'v' b'e' b't' b' ' b'D' b'r' b'i' b'v' b'e' b',' b' ' b'w' b'e'\n",
            " b'r' b'e' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "id": "erVoYFeVwvTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e1d2b6c-c2b8-4265-f394-42988cea5fa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'\"Text\"@\"Chapter\"@\"Book\"\\n\"THE BOY WHO LIVED  Mr. and Mrs. Dursley, of number four, Privet Drive, were '\n",
            "b\"proud to say that they were perfectly normal, thank you very much. They were the last people you'd ex\"\n",
            "b\"pect to be involved in anything strange or mysterious, because they just didn't hold with such nonsen\"\n",
            "b'se.  Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy '\n",
            "b'man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blond'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For training we need a dataset of (input, label) pairs. Where input and label are sequences. At each time step the input is the current character and the label is the next character.\n",
        "\n",
        "The <code>split_input_target</code> function takes a sequence as input, duplicates, and shifts it to align the input and label for each timestep:"
      ],
      "metadata": {
        "id": "OwKzugSsxLYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "split_input_target(list(\"Tensorflow\"))\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "id": "ng09QHSGwyCB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "030d0ad2-ab19-4cf9-acb0-8c952e3face9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'\"Text\"@\"Chapter\"@\"Book\"\\n\"THE BOY WHO LIVED  Mr. and Mrs. Dursley, of number four, Privet Drive, were'\n",
            "Target: b'Text\"@\"Chapter\"@\"Book\"\\n\"THE BOY WHO LIVED  Mr. and Mrs. Dursley, of number four, Privet Drive, were '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Create training batches</h2>\n",
        "We used tf.data to split the text into manageable sequences. But before feeding this data into the model, we need to shuffle the data and pack it into batches."
      ],
      "metadata": {
        "id": "uWNNN1wYxfuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "Lg5_JLblxWJs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bba0a5c-4a09-4405-c441-58db5b7865fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Build The Model</h1>\n",
        "This section defines the model as a keras.Model subclass (For details see Making new Layers and Models via subclassing).\n",
        "\n",
        "This model has three layers:\n",
        "<ul>\n",
        "  <li><code>tf.keras.layers.Embedding</code>: The input layer. A trainable lookup table that will map each character-ID to a vector with embedding_dim dimensions;</li>\n",
        "  <li><code>tf.keras.layers.GRU</code>: A type of RNN with size units=rnn_units (You can also use an LSTM layer here.)</li>\n",
        "  <li><code>tf.keras.layers.Dense</code>: The output layer, with vocab_size outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model.</li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "XWvJPLPIx1p0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x\n",
        "\n",
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "kpOtbBZ5xp9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Try the model</h1>\n",
        "Now run the model to see that it behaves as expected.\n",
        "First check the shape of the output:"
      ],
      "metadata": {
        "id": "6MBXoYiGyN1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "id": "IlMFJ1l2yL6N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ff3d9d0-05b4-4328-bfab-874635d51d9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 105) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "Mm89Pc-1Ut1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c01be7d-c62e-4d82-b017-dbad2fe886b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  26880     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  107625    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,072,809\n",
            "Trainable params: 4,072,809\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get actual predictions from the model we need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
        "using the first example in the batch, this gives us, at each timestep, a prediction of the next character index:"
      ],
      "metadata": {
        "id": "QR9a-yGJU-ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "sampled_indices"
      ],
      "metadata": {
        "id": "ZkggEVwZU5wY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "450c4ca1-475e-4d37-93ca-e3bc4364d715"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 72,  40,  53,  66,  47,  43,  80,  52,  78, 103,  58,  71,  96,\n",
              "        57,  22,  45,  64,  67,  87,  81,   1,  54,  86,  45, 101,  85,\n",
              "        67,  11,  96,   1,  79,   2, 103,   4,  62,   1,  52,  64,  96,\n",
              "        83,  54,   5,  30,  37,  28,  25,   9,  29,  65,  80,  99,  66,\n",
              "        94,  18,  21,  75,  18,  40,  25,  76,  60,  25,  58,  95,  95,\n",
              "        41,  44,  49,  28,   5,  83,  40,  83,  54,  92,  72,  67,  57,\n",
              "       103,  38,  85,  66,  60,  89,  67,  97,  74,  48,  37,  54,  36,\n",
              "        27,  18,  15,  12,  12,  45,  10,   8,  34])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoded text predicted by untrained model:\n"
      ],
      "metadata": {
        "id": "qXTNGBVnVSUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "id": "wg3ryi3fVGda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32c12e4c-b3b8-44be-89bb-551014796805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'ing the corridor ahead. Two enormous feet sticking out at the bottom and a loud puffing sound told t'\n",
            "\n",
            "Next Char Predictions:\n",
            " b'gGTaNJoSm\\xe2\\x80\\x9dYf\\xc2\\xa6X5L_bvp\\tUuL\\xe2\\x80\\x99tb)\\xc2\\xa6\\tn\\n\\xe2\\x80\\x9d!]\\tS_\\xc2\\xa6rU\"=D;8\\'<`o\\xe2\\x80\\x94a~14j1G8k[8Y\\xc2\\xa0\\xc2\\xa0HKP;\"rGrU|gbX\\xe2\\x80\\x9dEta[xb\\xc2\\xa8iODUC:1.**L(&A'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Train the model</h1>\n",
        "Given the previous RNN state, and the input this time step, predict the class of the next character.\n",
        "<h2>Attach an optimizer, and a loss function</h2>\n",
        "The standard <code>tf.keras.losses.sparse_categorical_crossentropy</code> loss function works in this case because it is applied across the last dimension of the predictions.\n",
        "\n",
        "Because our model returns logits, we need to set the from_logits flag."
      ],
      "metadata": {
        "id": "jUX43TNRVdFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "id": "YdjtU7qfVWbZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28c03ad4-5fbb-4a71-9d40-9c4704ebfbf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 105)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.651274, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A newly initialized model shouldn't be too sure of itself, the output logits should all have similar magnitudes. To confirm this we can check that the exponential of the mean loss is approximately equal to the vocabulary size. A much higher loss means the model is sure of its wrong answers, and is badly initialized:"
      ],
      "metadata": {
        "id": "qgRLi-RlV1pL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "id": "qEPkvmWDVuxh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16e33784-084b-4e9a-e384-14df208f48fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "104.71833"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We configure the training procedure using the <code>tf.keras.Model.compile</code> method. Use <code>tf.keras.optimizers.Adam</code> with default arguments and the loss function."
      ],
      "metadata": {
        "id": "s73s12hhV4-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "gQOtT1qiV3TC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Configure Checkpoints</h2>\n",
        "Use a tf.keras.callbacks.ModelCheckpoint to ensure that checkpoints are saved during training:"
      ],
      "metadata": {
        "id": "zo1KAh-EV9ca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "lopLf__XV6i9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Execute the training</h2>\n",
        "To keep training time reasonable, we used 10 epochs to train the model. In Colab, we set the runtime to GPU for faster training."
      ],
      "metadata": {
        "id": "J6pgF9vsWPtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#EPOCHS = 50\n",
        "EPOCHS = 10\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aJ9q8u_WDxD",
        "outputId": "246d49f3-5311-4343-d059-07bb00e77fc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "978/978 [==============================] - 183s 183ms/step - loss: 1.7541\n",
            "Epoch 2/10\n",
            "978/978 [==============================] - 180s 183ms/step - loss: 1.2134\n",
            "Epoch 3/10\n",
            "978/978 [==============================] - 181s 184ms/step - loss: 1.1282\n",
            "Epoch 4/10\n",
            "978/978 [==============================] - 179s 181ms/step - loss: 1.0839\n",
            "Epoch 5/10\n",
            "978/978 [==============================] - 178s 181ms/step - loss: 1.0530\n",
            "Epoch 6/10\n",
            "978/978 [==============================] - 179s 182ms/step - loss: 1.0292\n",
            "Epoch 7/10\n",
            "978/978 [==============================] - 180s 182ms/step - loss: 1.0099\n",
            "Epoch 8/10\n",
            "978/978 [==============================] - 181s 183ms/step - loss: 0.9937\n",
            "Epoch 9/10\n",
            "978/978 [==============================] - 180s 183ms/step - loss: 0.9810\n",
            "Epoch 10/10\n",
            "978/978 [==============================] - 180s 182ms/step - loss: 0.9707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Generate text</h1>\n",
        "The simplest way to generate text with this model is to run it in a loop, and keep track of the model's internal state as you execute it.<br>\n",
        "Each time you call the model you pass in some text and an internal state. The model returns a prediction for the next character and its new state. Pass the prediction and state back in to continue generating text.\n",
        "\n",
        "The following makes a single step prediction:"
      ],
      "metadata": {
        "id": "J8AxIH-pVOZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "seRFW5wzWOyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "XPixnmNaVl3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We run it in a loop to generate some text. Looking at the generated text, we see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
      ],
      "metadata": {
        "id": "MiLk6_irVZwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['Character_Name:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "id": "-WcC5oMbVYJT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "843472ea-74e9-4982-9fbb-004f284458c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Character_Name: this married last night, the recentmark thing are coming out of here. And if you want to.E.W, Master,\\\" bleated Professor McGonagall. \\\"I won't be laughing about how they really know. Attempting to keep getting really annoyed. I should prefer not yours, it's not so surprise!\\\"  She was Mrs Weasley pointed the boy, looking up and looking after him.  \\\"You seek up here,' said Harry, looking strained and did not betray the newspaper on her and he glanced around the office, she pulled at all thicket.\\\"Grangest attack,\\\" said Ron, and his eyes laughed, flanks into her hand. \\\"The fifth-yard,' said Harry quietly. \\\"It whike has been in a new birth of witch here in a corner and a body for you, I don't know Voldemort stuff about that house, most of them wouldn't have, is my grip in Professor Flitwick and when he called the third time.\\\"Hermione thirted him back to shift sounds of grave. \\\"It'll be working soonts. . . . The whole vatcher?\\\"\\\"That's great!\\\" said Dumbledore quietly.Harry's scra \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.8080055713653564\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recommendations to improve results:\n",
        "<ul>\n",
        "  <li>Train it for longer (try EPOCHS = 30).</li>\n",
        "  <li>Experiment with a different start string, try adding another RNN layer to improve the model's accuracy, or adjust the temperature parameter to generate more or less random predictions.</li>\n",
        "  <li>To generate text faster the easiest thing you can do is batch the text generation. In the example below the model generates 5 outputs in about the same time it took to generate 1 above.</li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "wONo0ImkVtId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['name:', 'name:', 'name:', 'name:', 'name:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "id": "ijNGYT8_VhAX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79c49034-a120-4c3d-8923-89b864dd1d94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'name:  You-Know-Who and Inslaved after all.\\\\\"\\xc2\\xa0That\\'s him! We\\'ve heard it\\'s Funny, you - who\\'d agree with the magical combati,\\\\\" she said. \\\\\"Calm \\xc2\\xa8C \\\\\"\\\\\"We\\'ll take raving in that woman who really counted him.\\\\\"Germis revers! Of course ... I expect. I\\'m going posset. So...\\\\\"\\\\\"You don\\'t understand,\\\\\" said Hermione stridingly, scanning up ang into the rain. \\\\\"What if one of your means could do when alotter rightly wants it?\\\\\"\\\\\"We know it, Harry, boybing and turning ontable owh waft in the house?\\\\\"Perfect him an old pause in his life: Horcruxes had occurred to press his mouth wide. Hagrids rampar, and there were only Lee Jontant and Dad answerbelled; but did not think it. \\'POTT tell Professor Trelawney?\\'\\xc2\\xa0 \\xc2\\xa0 \\'What about you?\\'\\xc2\\xa0\\'You cave with \\'moment!\\'\\xc2\\xa0 \\xc2\\xa0 The sword was extinguished and stumbling on the other fire boardou-looking raping on the Room of Felix as Decarters. Harry listened those conversations were Battim Moody had discussed the Cloak on tut on orange, dimatous?\\\\\"The Crumple whis got him'\n",
            " b'name: Jinxes, the assary-second tity in his purple-lasted mirror, \\\\\"but we\\'ll have leave memory. He wanted to kill me, I walk every word. One of those a loud battle and wrong-time shabby\\'s beds and sealed Mrs. Cloak draft a little. \\\\\"I never come back, then!\\\\\"\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0She dragged back the corridor and Harry saw at Harry\\'s. Harry could not be alive, each and broke inside his warm left to eight of the dormitory wood, and his herops the charm weighed his shoulders and the voice fell into the window, the caretaker knew had appeared in, and was in the entrance hall.\\\\\"I\\'m really sorry, Erno ?ensideraking and of the eve is unpleasant houses,\\\\\" said Dumbledore quietly. \\\\\"Impedimenta\\'s,\\' said Snape fiercely. \\\\\"What we tried to do with their chorses, Are they?\\\\\" demedded Snape, and he looked close to her every direction. The elf looked quite at a rude everyther in places hidden and the wand in front of them were flaining in Harry\\'s trunk and was full of thick attack: He had vanished, lowered her cage. Harr'\n",
            " b'name: The Carrows and Harry felt his eyes glittering in his arm, and there were gasps, as though he had asked each other, it was hagrily and proud of himself.  Voldemort had stumbled out of his eye; the iden hiss and rang and the price floon; it was sleeping with the enchanted carilities of the picture from which thick. He looked back at his own son as he repleced each well; the idea had lifted his palm and breaking up and choking. It slowed down, like Hagrid, but yet Winky was stupidity. Half of their refused to fight. He had been on his suit of arm\\'s tiny twin card by air. Harry raised his eyebrows.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\\\\"Aharble now, don\\'t go apparently known at Hogwarts, I am a wizard for ages!\\\\\" shightlessamed Professor McGonagall. \\\\\"But what are no reason who knows you\\'re like man?\\\\\"Xenophilius crushed the wiccept \\\\\"be which a S-cip of guard, as though,\\\\\" muttered Aberforth. \\\\\"There\\'s only a dreadful headless in luck, you spot? Everything useful not firmly. I got a bit of photosoldrops, throwing the las'\n",
            " b'name: There were way came to him, more peohless, in the Deathly Hallows on which he found himself around the water; Harry recognised the Grawp write his own darkness, purely achieved his way apart and wrenched his wand, for a moment he held out all his arm and Hermione jumped around his grip; Hedwig\\'s cage was wide, two\\'s temptation! Do you use it.\\\\\"Now, Harry, you\\'re out of the Those points if you don\\'t even recoge yours, I\\'m sure,\\\\\" said Harry out-oner the flashes.\\\\\"I don\\'t know where she hair worked for me,\\\\\" raged Harry. \\\\\"Whereful not ?quot;\\xc2\\xa0\\\\\"Someone thinks there\\'s something in your slipy little hand?\\\\\" roared another, curling it with apprehension.  \\\\\"Can you tell our fits,\\\\\" she squeaked, now magically magnified upon his cheeks as the underground dark landing of backwords like an ancient? \\\\\"Needs they say.\\\\\" He pointed brough what looked like evilly light! Is Hufflepuffs just new awartment?\\\\\"\\\\\"Wo do with a fake Day want, was finding it!\\\\\" cried Hermione, and as a snake surface of all'\n",
            " b'name: It was, these elder, including an imagination about Bellatrix in a tray from Harry.\\\\\"I know what he\\'s such an, dear,\\\\\" said Hermione, \\\\\"no prous on Voldemort\\'s righ\\'s powdered rather than anybody. Listen \\xc2\\xa8C large than fishicned Kit captain.\\\\\"New, no, Professor, fine,\\\\\" said Kingin Riddle, Albus jumped down the nedden staring no one an his scar burning in his cupboard. \\\\\"Still.His father Scum \\xc2\\xa8C \\\\\"\\\\\"I\\'m going to return,\\\\\" Harry called and, defiant duels, \\\\\"this long match fails permitteen . . .\\xc2\\xa0\"@36@7\\n\"Harry, who had been waiting off his bedroom door. No, I am here - you\\'ve got the werewolf of everything worse! It\\'s again, you think Dumbledore disobical house was coming in my book since the three brothers \\xc2\\xa8C \\\\\"\\\\\"This is a village, Draco\\'s Legilimency!\\\\\" said Harry in disbelief. \\\\\"The age when I packed from here.\\\\\"He sat up at the sword, bewilling horribly, but no such thing he had arrived at Hermione, who was behind the rushions were House at a number of funny golden light said Dumbled'], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 5.6485137939453125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Export the generator</h2>\n",
        "This single-step model can easily be saved and restored, allowing you to use it anywhere a tf.saved_model is accepted.\n"
      ],
      "metadata": {
        "id": "n3S6cepXj7WV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "id": "k-YR2RLIVw5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0325c6d-ede7-4b20-a789-243e6b71157f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f6ed0170790>, because it is not built.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant(['name:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "id": "VeMPutSdkC5z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4b77f10-8ce4-43ee-ab40-8093e913c92a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name: It was an unpleasant swelve facts and charms, he doubted that she can imare the cup of teachers sti\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "New Attempt"
      ],
      "metadata": {
        "id": "Sjbt7xWwVALy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre processing steps are importing the Aesop Fables json files. Final Submission?"
      ],
      "metadata": {
        "id": "pQOfazYaVC-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request, json \n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import io\n",
        "\n",
        "#path = keras.utils.get_file(\n",
        "#    \"nietzsche.txt\", origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\"\n",
        "#)\n",
        "#with io.open(path, encoding=\"utf-8\") as f:\n",
        "#    text = f.read().lower()\n",
        "#text = text.replace(\"\\n\", \" \")  # We remove newlines chars for nicer display\n",
        "#print(\"Corpus length:\", len(text))\n",
        "\n",
        "'''imported the aesops json file'''\n",
        "with urllib.request.urlopen(\"https://raw.githubusercontent.com/itayniv/aesop-fables-stories/master/public/aesopFables.json\") as url:\n",
        "    data_dict = json.loads(url.read().decode())\n",
        "text=\"\"\n",
        "for key, value in data_dict.items():\n",
        "  for story in value:\n",
        "    temp = dict(story)\n",
        "    text += str(temp[\"story\"]) + \" \"\n",
        "print(text[:100])\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "print(\"Total chars:\", len(chars))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "# cut the text in semi-redundant sequences of maxlen characters\n",
        "maxlen = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i : i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "print(\"Number of sequences:\", len(sentences))\n",
        "\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1\n",
        "\n",
        "\n",
        "\n",
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype(\"float64\")\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "#epochs = 40\n",
        "#batch_size = 128\n",
        "\n",
        "def custom_train(model, epochs, batch_size):\n",
        "  for epoch in range(epochs):\n",
        "      model.fit(x, y, batch_size=batch_size, epochs=1)\n",
        "      print()\n",
        "      print(\"Generating text after epoch: %d\" % epoch)\n",
        "\n",
        "      start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "      for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "          print(\"...Diversity:\", diversity)\n",
        "\n",
        "          generated = \"\"\n",
        "          sentence = text[start_index : start_index + maxlen]\n",
        "          print('...Generating with seed: \"' + sentence + '\"')\n",
        "\n",
        "          for i in range(400):\n",
        "              x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "              for t, char in enumerate(sentence):\n",
        "                  x_pred[0, t, char_indices[char]] = 1.0\n",
        "              preds = model.predict(x_pred, verbose=0)[0]\n",
        "              next_index = sample(preds, diversity)\n",
        "              next_char = indices_char[next_index]\n",
        "              sentence = sentence[1:] + next_char\n",
        "              generated += next_char\n",
        "\n",
        "          print(\"...Generated: \", generated)\n",
        "          print()\n",
        "\n",
        "def custom_train_no_batch(model, epochs):\n",
        "  for epoch in range(epochs):\n",
        "      model.fit(x, y, epochs=1)\n",
        "      print()\n",
        "      print(\"Generating text after epoch: %d\" % epoch)\n",
        "\n",
        "      start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "      for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "          print(\"...Diversity:\", diversity)\n",
        "\n",
        "          generated = \"\"\n",
        "          sentence = text[start_index : start_index + maxlen]\n",
        "          print('...Generating with seed: \"' + sentence + '\"')\n",
        "\n",
        "          for i in range(400):\n",
        "              x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "              for t, char in enumerate(sentence):\n",
        "                  x_pred[0, t, char_indices[char]] = 1.0\n",
        "              preds = model.predict(x_pred, verbose=0)[0]\n",
        "              next_index = sample(preds, diversity)\n",
        "              next_char = indices_char[next_index]\n",
        "              sentence = sentence[1:] + next_char\n",
        "              generated += next_char\n",
        "\n",
        "          print(\"...Generated: \", generated)\n",
        "          print()\n",
        "\n",
        "new_model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(maxlen, len(chars))),\n",
        "        #layers.LSTM(512, return_sequences=True),\n",
        "        #layers.LSTM(512, return_sequences=True),\n",
        "        #layers.LSTM(512),\n",
        "        layers.LSTM(128),\n",
        "        #layers.Dense(256, activation=\"softmax\"),\n",
        "        #layers.Dense(128, activation=\"softmax\"),\n",
        "        layers.Dense(len(chars), activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "optimizer = keras.optimizers.RMSprop(learning_rate=0.01)\n",
        "new_model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
        "\n",
        "epochs = 40\n",
        "batch_size = 128\n",
        "\n",
        "#custom_train(new_model, epochs, batch_size)\n",
        "custom_train_no_batch(new_model, epochs)\n",
        "\n",
        "\n",
        "#additional training quietly\n",
        "new_model.fit(x, y, epochs=epochs)#batch_size=batch_size,\n",
        "print()\n",
        "#print(\"Generating text after epoch: %d\" % epoch)\n",
        "\n",
        "start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "  print(\"...Diversity:\", diversity)\n",
        "\n",
        "  generated = \"\"\n",
        "  sentence = text[start_index : start_index + maxlen]\n",
        "  print('...Generating with seed: \"' + sentence + '\"')\n",
        "\n",
        "  for i in range(400):\n",
        "      x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "      for t, char in enumerate(sentence):\n",
        "          x_pred[0, t, char_indices[char]] = 1.0\n",
        "      preds = new_model.predict(x_pred, verbose=0)[0]\n",
        "      next_index = sample(preds, diversity)\n",
        "      next_char = indices_char[next_index]\n",
        "      sentence = sentence[1:] + next_char\n",
        "      generated += next_char\n",
        "\n",
        "  print(\"...Generated: \", generated)\n",
        "  print()\n",
        "\n",
        "  start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "  print(\"...Diversity:\", diversity)\n",
        "\n",
        "  generated = \"\"\n",
        "  sentence = text[start_index : start_index + maxlen]\n",
        "  print('...Generating with seed: \"' + sentence + '\"')\n",
        "\n",
        "  for i in range(400):\n",
        "      x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "      for t, char in enumerate(sentence):\n",
        "          x_pred[0, t, char_indices[char]] = 1.0\n",
        "      preds = new_model.predict(x_pred, verbose=0)[0]\n",
        "      next_index = sample(preds, diversity)\n",
        "      next_char = indices_char[next_index]\n",
        "      sentence = sentence[1:] + next_char\n",
        "      generated += next_char\n",
        "\n",
        "  print(\"...Generated: \", generated)\n",
        "  print()"
      ],
      "metadata": {
        "id": "YBMqbW5CNmrC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}