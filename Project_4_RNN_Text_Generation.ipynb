{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project 4-RNN-Text-Generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TimothyJan/RNN_Text_Generation/blob/main/Project_4_RNN_Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spring 2022 - CPSC 585-section-13883\n",
        "Project 4 - Recurrent Neural Networks Text Generation\n",
        "Sean Javiya\n",
        "Timothy Jan\n",
        "Timothy Kheang\n",
        "\n",
        "In Project 4, we use an RNN-based language model to generate text for a creative application.\n",
        "\n",
        "Goals for this project are:\n",
        "<ul>\n",
        "  <li>Reading about how RNNs can be used to generate text, and examining several different applications after training on different text corpora.</li>\n",
        "  <li>Using a multi-layer RNN to train and sample from a character-level language model.</li>\n",
        "  <li>Adapting and reusing published model code.</li>\n",
        "  <li>Having some fun.</li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "0JqoLNUQL6oU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eCr_lbTxLs6e"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import urllib.request, json "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the Elder Scroll's or Shakespearean dataset and read the data."
      ],
      "metadata": {
        "id": "S4ctoJPB_paf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#'''imported the elder scrolls json file'''\n",
        "#with urllib.request.urlopen(\"https://raw.githubusercontent.com/hmi-utwente/video-game-text-corpora/master/The%20Elder%20Scrolls/data/imperial_library_20200626.json\") as url:\n",
        "#    data_dict = json.loads(url.read().decode())\n",
        "#text=\"\"\n",
        "#for key in data_dict:\n",
        "#  text += data_dict[key]['description'] +' ' + data_dict[key]['title'] + ' ' + data_dict[key]['text']\n",
        "#text[:100]\n",
        "\n",
        "#'''imported the aesops json file'''\n",
        "#with urllib.request.urlopen(\"https://raw.githubusercontent.com/itayniv/aesop-fables-stories/master/public/aesopFables.json\") as url:\n",
        "#    data_dict = json.loads(url.read().decode())\n",
        "#text=\"\"\n",
        "#for key, value in data_dict.items():\n",
        "#  for story in value:\n",
        "#    temp = dict(story)\n",
        "#    text += str(temp[\"story\"]) + \" \"\n",
        "#print(text[:100])\n",
        "\n",
        "# '''imported the Harry Potter text files'''\n",
        "text=\"\"\n",
        "with urllib.request.urlopen(\"https://raw.githubusercontent.com/ErikaJacobs/Harry-Potter-Text-Mining/master/Book%20Text/HPBook1.txt\") as url:\n",
        "    text += url.read().decode()\n",
        "with urllib.request.urlopen(\"https://raw.githubusercontent.com/ErikaJacobs/Harry-Potter-Text-Mining/master/Book%20Text/HPBook2.txt\") as url:\n",
        "    text += url.read().decode()\n",
        "with urllib.request.urlopen(\"https://raw.githubusercontent.com/ErikaJacobs/Harry-Potter-Text-Mining/master/Book%20Text/HPBook3.txt\") as url:\n",
        "    text += url.read().decode()\n",
        "with urllib.request.urlopen(\"https://raw.githubusercontent.com/ErikaJacobs/Harry-Potter-Text-Mining/master/Book%20Text/HPBook4.txt\") as url:\n",
        "    text += url.read().decode()\n",
        "with urllib.request.urlopen(\"https://raw.githubusercontent.com/ErikaJacobs/Harry-Potter-Text-Mining/master/Book%20Text/HPBook5.txt\") as url:\n",
        "    text += url.read().decode()\n",
        "with urllib.request.urlopen(\"https://raw.githubusercontent.com/ErikaJacobs/Harry-Potter-Text-Mining/master/Book%20Text/HPBook6.txt\") as url:\n",
        "    text += url.read().decode()\n",
        "with urllib.request.urlopen(\"https://raw.githubusercontent.com/ErikaJacobs/Harry-Potter-Text-Mining/master/Book%20Text/HPBook7.txt\") as url:\n",
        "    text += url.read().decode()\n",
        "\n",
        "# '''Shakespearen dataset'''\n",
        "# path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "# # Read, then decode for py2 compat.\n",
        "# text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# # length of text is the number of characters in it\n",
        "# print(f'Length of text: {len(text)} characters')\n",
        "# print(text[:100])"
      ],
      "metadata": {
        "id": "PjHAQwPv2ZQj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the Data"
      ],
      "metadata": {
        "id": "hIQCraeN_v7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODEUkCSo2uCf",
        "outputId": "cb2ffbc2-2a36-417b-ac67-0fd2be431eb7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "104 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Process the text</h1>\n",
        "<h2>Vectorize the text.</h2>\n",
        "Before training, you need to convert the strings to a numerical representation.\n",
        "The tf.keras.layers.StringLookup layer can convert each character into a numeric ID. It just needs the text to be split into tokens first."
      ],
      "metadata": {
        "id": "4c9STpH-_1qO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6zWduFuuaIs",
        "outputId": "72f46b98-8467-4ce3-e5c6-fae8803630da"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the tf.keras.layers.StringLookup layer. It onverts from tokens to character IDs:"
      ],
      "metadata": {
        "id": "wwsioWOgu7Pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)\n",
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztbeaqT886Ni",
        "outputId": "5d4529d8-a4c4-4d37-993e-5ca67b6869fc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[66, 67, 68, 69, 70, 71, 72], [89, 90, 91]]>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This layer recovers the characters from the vectors of IDs, and returns them as a tf.RaggedTensor of characters:"
      ],
      "metadata": {
        "id": "YMRHxp5gvSsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHnRKiJZuEN_",
        "outputId": "b23e8ef9-181d-4c49-8ee0-527bddac4df2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use tf.strings.reduce_join to join the characters back into strings."
      ],
      "metadata": {
        "id": "fQsbjGoQvlZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wU3pOFakvUMx",
        "outputId": "c407d79b-ef1d-431e-a1ea-25c5ca790c46"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n"
      ],
      "metadata": {
        "id": "jQJmm671vqst"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>The Prediction Task</h2>\n",
        "Given a character, or a sequence of characters, what is the most probable next character? This is the task you're training the model to perform. The input to the model will be a sequence of characters, and you train the model to predict the output—the following character at each time step.\n",
        "\n",
        "Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?\n",
        "\n",
        "<h2>Create training examples and targets</h2>\n",
        "Next divide the text into example sequences. Each input sequence will contain seq_length characters from the text.\n",
        "\n",
        "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
        "\n",
        "So break the text into chunks of seq_length+1. For example, say seq_length is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
        "\n",
        "To do this first use the tf.data.Dataset.from_tensor_slices function to convert the text vector into a stream of character indices."
      ],
      "metadata": {
        "id": "ygvRQ68jwFGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n07gqUwPvtCD",
        "outputId": "da8df1ab-712d-4ae5-c35d-2aed5078c5e8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(6325064,), dtype=int64, numpy=array([ 5, 53, 70, ..., 33, 24,  2])>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wywrnl1_wj4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWdmxTW4wUQ4",
        "outputId": "154a5cbe-0384-4623-e93e-0a5fddfec583"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"\n",
            "T\n",
            "e\n",
            "x\n",
            "t\n",
            "\"\n",
            "@\n",
            "\"\n",
            "C\n",
            "h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The batch method lets you easily convert these individual characters to sequences of the desired size."
      ],
      "metadata": {
        "id": "kqaTSg_kwsg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsYC0q3OwhPk",
        "outputId": "5d174456-de39-4527-b3b7-2cfe889564fe"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'\"' b'T' b'e' b'x' b't' b'\"' b'@' b'\"' b'C' b'h' b'a' b'p' b't' b'e'\n",
            " b'r' b'\"' b'@' b'\"' b'B' b'o' b'o' b'k' b'\"' b'\\n' b'\"' b'T' b'H' b'E'\n",
            " b' ' b'B' b'O' b'Y' b' ' b'W' b'H' b'O' b' ' b'L' b'I' b'V' b'E' b'D'\n",
            " b' ' b' ' b'M' b'r' b'.' b' ' b'a' b'n' b'd' b' ' b'M' b'r' b's' b'.'\n",
            " b' ' b'D' b'u' b'r' b's' b'l' b'e' b'y' b',' b' ' b'o' b'f' b' ' b'n'\n",
            " b'u' b'm' b'b' b'e' b'r' b' ' b'f' b'o' b'u' b'r' b',' b' ' b'P' b'r'\n",
            " b'i' b'v' b'e' b't' b' ' b'D' b'r' b'i' b'v' b'e' b',' b' ' b'w' b'e'\n",
            " b'r' b'e' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erVoYFeVwvTv",
        "outputId": "bdee8ba7-fb03-4a9a-bfe0-4b702d20ea70"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'\"Text\"@\"Chapter\"@\"Book\"\\n\"THE BOY WHO LIVED  Mr. and Mrs. Dursley, of number four, Privet Drive, were '\n",
            "b\"proud to say that they were perfectly normal, thank you very much. They were the last people you'd ex\"\n",
            "b\"pect to be involved in anything strange or mysterious, because they just didn't hold with such nonsen\"\n",
            "b'se.  Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy '\n",
            "b'man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blond'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For training you'll need a dataset of (input, label) pairs. Where input and label are sequences. At each time step the input is the current character and the label is the next character.\n",
        "\n",
        "Here's a function that takes a sequence as input, duplicates, and shifts it to align the input and label for each timestep:"
      ],
      "metadata": {
        "id": "OwKzugSsxLYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "split_input_target(list(\"Tensorflow\"))\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ng09QHSGwyCB",
        "outputId": "f80e2b5e-16d9-44fc-8091-6a061055c6d1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'\"Text\"@\"Chapter\"@\"Book\"\\n\"THE BOY WHO LIVED  Mr. and Mrs. Dursley, of number four, Privet Drive, were'\n",
            "Target: b'Text\"@\"Chapter\"@\"Book\"\\n\"THE BOY WHO LIVED  Mr. and Mrs. Dursley, of number four, Privet Drive, were '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Create training batches</h2>\n",
        "You used tf.data to split the text into manageable sequences. But before feeding this data into the model, you need to shuffle the data and pack it into batches."
      ],
      "metadata": {
        "id": "uWNNN1wYxfuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lg5_JLblxWJs",
        "outputId": "d4e5acc5-c940-4778-b8d9-687596498348"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Build The Model</h1>\n",
        "This section defines the model as a keras.Model subclass (For details see Making new Layers and Models via subclassing).\n",
        "\n",
        "This model has three layers:\n",
        "<ul>\n",
        "  <li>tf.keras.layers.Embedding: The input layer. A trainable lookup table that will map each character-ID to a vector with embedding_dim dimensions;</li>\n",
        "  <li>tf.keras.layers.GRU: A type of RNN with size units=rnn_units (You can also use an LSTM layer here.)</li>\n",
        "  <li>tf.keras.layers.Dense: The output layer, with vocab_size outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model.</li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "XWvJPLPIx1p0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x\n",
        "\n",
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "kpOtbBZ5xp9b"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Try the model</h1>\n",
        "Now run the model to see that it behaves as expected.\n",
        "\n",
        "First check the shape of the output:"
      ],
      "metadata": {
        "id": "6MBXoYiGyN1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlMFJ1l2yL6N",
        "outputId": "4a9db973-57eb-4885-95e6-04af13f86c4e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 105) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mm89Pc-1Ut1b",
        "outputId": "bffefc87-e8a0-4ff3-8a03-d47895b7812b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  26880     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  107625    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,072,809\n",
            "Trainable params: 4,072,809\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get actual predictions from the model you need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
        "using the first example in the batch, this gives us, at each timestep, a prediction of the next character index:"
      ],
      "metadata": {
        "id": "QR9a-yGJU-ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkggEVwZU5wY",
        "outputId": "308bc254-b333-4859-c4f4-83e613e29fbc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 88,  15,  50,  60,  58,  48,  62,  50,  13,  77,  68,  51,  58,\n",
              "         4,  26,  95,  77,  82,  86,   4,  98,  81,  82,  89,  39, 100,\n",
              "        46,  90,  87,  16,  14,  41,  35,  24,  47,   2,  81,  19,  22,\n",
              "         5,  90,  98,  91,   3,  60,   5,  15,  27,  54,  86,  29,  76,\n",
              "        83,  18,  62,  72,  21,  72,  32,   7,  69,  96,  23,  59,  28,\n",
              "        16,  11,  28,  15,  40,  45, 100,  99,  38,  89,  12,  50,  71,\n",
              "        94,  92,  20,  64,  79,  38,  45,  67,  57,  23, 102,  53,  94,\n",
              "        60,  18,  83,  63,  35,  29,  58,  30, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoded text predicted by untrained model:\n"
      ],
      "metadata": {
        "id": "qXTNGBVnVSUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wg3ryi3fVGda",
        "outputId": "e2ed6998-06a5-4bb9-9072-9542c6a39797"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b' shoulders to stay on.  \\\\\"Do you not see that unicorn?\\\\\" Firenze bellowed at Bane. \\\\\"Do you not unde'\n",
            "\n",
            "Next Char Predictions:\n",
            " b'w.Q[YO]Q,lcRY!9\\xc2\\xa0lqu!\\xe2\\x80\\x93pqxF\\xe2\\x80\\x98Myv/-HB7N\\np25\"y\\xe2\\x80\\x93z [\".:Uu<kr1]g4g?%d\\xc2\\xa66Z;/);.GL\\xe2\\x80\\x98\\xe2\\x80\\x94Ex*Qf~|3_nELbX6\\xe2\\x80\\x9cT~[1r^B<Y=\\xe2\\x80\\x98'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Train the model</h1>\n",
        "Given the previous RNN state, and the input this time step, predict the class of the next character.\n",
        "<h2>Attach an optimizer, and a loss function</h2>\n",
        "The standard tf.keras.losses.sparse_categorical_crossentropy loss function works in this case because it is applied across the last dimension of the predictions.\n",
        "\n",
        "Because our model returns logits, you need to set the from_logits flag."
      ],
      "metadata": {
        "id": "jUX43TNRVdFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdjtU7qfVWbZ",
        "outputId": "fbb1072d-a4c9-4e23-954d-30c2d010a71f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 105)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.656226, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A newly initialized model shouldn't be too sure of itself, the output logits should all have similar magnitudes. To confirm this you can check that the exponential of the mean loss is approximately equal to the vocabulary size. A much higher loss means the model is sure of its wrong answers, and is badly initialized:"
      ],
      "metadata": {
        "id": "qgRLi-RlV1pL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEPkvmWDVuxh",
        "outputId": "238cf572-c28c-42d1-956f-4cbc648ac734"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "105.23818"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure the training procedure using the tf.keras.Model.compile method. Use tf.keras.optimizers.Adam with default arguments and the loss function."
      ],
      "metadata": {
        "id": "s73s12hhV4-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "gQOtT1qiV3TC"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Configure Checkpoints</h2>\n",
        "Use a tf.keras.callbacks.ModelCheckpoint to ensure that checkpoints are saved during training:"
      ],
      "metadata": {
        "id": "zo1KAh-EV9ca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "lopLf__XV6i9"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Execute the training</h2>\n",
        "To keep training time reasonable, use 10 epochs to train the model. In Colab, set the runtime to GPU for faster training."
      ],
      "metadata": {
        "id": "J6pgF9vsWPtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#EPOCHS = 50\n",
        "EPOCHS = 20\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aJ9q8u_WDxD",
        "outputId": "df3432c1-30c9-486e-eecd-dffce00d4907"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "978/978 [==============================] - 139s 133ms/step - loss: 1.7743\n",
            "Epoch 2/20\n",
            "978/978 [==============================] - 131s 132ms/step - loss: 1.2180\n",
            "Epoch 3/20\n",
            "978/978 [==============================] - 130s 132ms/step - loss: 1.1303\n",
            "Epoch 4/20\n",
            "978/978 [==============================] - 131s 132ms/step - loss: 1.0845\n",
            "Epoch 5/20\n",
            "978/978 [==============================] - 132s 133ms/step - loss: 1.0525\n",
            "Epoch 6/20\n",
            "978/978 [==============================] - 132s 133ms/step - loss: 1.0275\n",
            "Epoch 7/20\n",
            "978/978 [==============================] - 130s 132ms/step - loss: 1.0066\n",
            "Epoch 8/20\n",
            "978/978 [==============================] - 131s 132ms/step - loss: 0.9901\n",
            "Epoch 9/20\n",
            "978/978 [==============================] - 132s 133ms/step - loss: 0.9763\n",
            "Epoch 10/20\n",
            "978/978 [==============================] - 131s 132ms/step - loss: 0.9650\n",
            "Epoch 11/20\n",
            "978/978 [==============================] - 130s 131ms/step - loss: 0.9571\n",
            "Epoch 12/20\n",
            "978/978 [==============================] - 129s 130ms/step - loss: 0.9509\n",
            "Epoch 13/20\n",
            "978/978 [==============================] - 129s 131ms/step - loss: 0.9467\n",
            "Epoch 14/20\n",
            "978/978 [==============================] - 129s 130ms/step - loss: 0.9446\n",
            "Epoch 15/20\n",
            "978/978 [==============================] - 129s 131ms/step - loss: 0.9435\n",
            "Epoch 16/20\n",
            "978/978 [==============================] - 130s 131ms/step - loss: 0.9438\n",
            "Epoch 17/20\n",
            "978/978 [==============================] - 130s 131ms/step - loss: 0.9459\n",
            "Epoch 18/20\n",
            "978/978 [==============================] - 131s 132ms/step - loss: 0.9482\n",
            "Epoch 19/20\n",
            "978/978 [==============================] - 130s 131ms/step - loss: 0.9536\n",
            "Epoch 20/20\n",
            "978/978 [==============================] - 130s 131ms/step - loss: 0.9593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Generate text</h1>\n",
        "The simplest way to generate text with this model is to run it in a loop, and keep track of the model's internal state as you execute it.<br>\n",
        "Each time you call the model you pass in some text and an internal state. The model returns a prediction for the next character and its new state. Pass the prediction and state back in to continue generating text.\n",
        "\n",
        "The following makes a single step prediction:"
      ],
      "metadata": {
        "id": "J8AxIH-pVOZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "seRFW5wzWOyX"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "XPixnmNaVl3L"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run it in a loop to generate some text. Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
      ],
      "metadata": {
        "id": "MiLk6_irVZwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['Character_Name:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WcC5oMbVYJT",
        "outputId": "c67db0d3-e8eb-4c9d-e686-713bfae91674"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Character_Name: I told him, but the more Horcrux Dumbledore, isn't happening was Eutes. He thought that Lily and James's Muggles? It'll stop frightening his magic, like you — oh! If you don't understand how much of still having runing you our eyes now, wouldn't he? Harry clutched her close to feed Voldemort but as he could feel his very girly-white thick suffining ever laughing, not there!\\\" she said quickly. \\\"No wonder Sester Bellatrix's wait. Cool with him, sir . .  They could not betrare contribute for hours. It sounded if he had going to look into Hogsmeade. Malfoy was racing it beyond one eye, dawn, the burnished coat with Death Eaters through his scar hunting in passagged him for his cloak in child. All at the front page, close to them, I told, Albus didn't know what happened to your friend! And the gray home then recounting Dumbledore's power in front of Early in lines for the last normal.\\\"\\\"'What the night's travel? But how are you getting awfust things!\\\" Hermione muttered. \\\"The evidence  \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 5.11844801902771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The easiest thing you can do to improve the results is to train it for longer (try EPOCHS = 30).\n",
        "\n",
        "You can also experiment with a different start string, try adding another RNN layer to improve the model's accuracy, or adjust the temperature parameter to generate more or less random predictions.\n",
        "\n",
        "If you want the model to generate text faster the easiest thing you can do is batch the text generation. In the example below the model generates 5 outputs in about the same time it took to generate 1 above."
      ],
      "metadata": {
        "id": "wONo0ImkVtId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['name:', 'name:', 'name:', 'name:', 'name:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijNGYT8_VhAX",
        "outputId": "28188c4e-53d3-4b3a-feaa-0f87cce49b3b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'name: Every tince lack further advice now that he was going to scream over the stories staring at Ginny. \\\\\"Hermione\\'s, but she did not know what the place the babythought and perhaps even returned to the twilit.  \\\\\"We hat, let me show me, but not so how they think now I want you to work out how to. Accio Curse?\\\\\"  The water was shallow and broke, he had currently shocked whenever the door behind him.He looked at him for anything that Harry saw her. \\'It\\'s our had the broken the Trace, which memories . . .\\'\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0The large was further unknown that McLaggen, the wand filled them before gotten that nearly exchant snow badly seemed new one who had lost his chest, a bundle of golds in retricute within repeating tradit, Griphook. Plouds lashing generalts of armor wished that it was only when he wasn\\'t the Felix Felicise, it\\'s lovely with Ron home from the Ministry that she could be over; he saw a password walk back out of the paper.The rark on stone floors of the gate, expanded now, and once he seemed'\n",
            " b\"name: He got into side antificors, leaving the barmings. At once by another witch. He chucked his hand. Professor Umbridge should sit down behind his own stone-grownded hand, his mother was more longing to him if Dumbledore wanted to give the rest of my boof human being leg to the Death Eaters... muggled horses, but still knowing what he was early that somebody opened itself upon him that there was no hope to their aftem, but something that the Ministry with the fame of it. The book was quite still at light as the began to run, trying to extract him, pointed it at the Halfoyes Anster the Polyjuice Potion was clawing with laughter. They gazed away from the Great Hall, and hurried off to fetch a color quietly smile, coming through a glimpse of shelm. Harry launched her arms: and a banging of rays of raising as far away, by wearing the living room and true Hedwig.\\xc2\\xa0 \\xc2\\xa0 'Nice, stuff!' Harry's heart bodied. 'I - would be, and asked why people will have forced them? But we reminded him for explain.\"\n",
            " b'name: its best match jinx would be the sword seemed murmuring so.  Madame fell jackets between them: the Snitch, then the pantide that reholved a long laugh more intensittering heapture, which was a big pluchane door and, to soo loudly to aftho?\\\\\"\\\\\"It\\'s not for house, I was under the sword.\\\\\"\\\\\"What bin that,\\\\\" he added, wirely causing to refuse. \\\\\"How do we kill her so like yeh?\\\\\" Ron asked. \\\\\"I pot up. She wouldn\\'t eat the truth! Now, I\\'m twenty of the wardrobe as much as you could,\\' said Auntie McGonagall. \\'So he caused my school this everyone unbeatent, does seem to know, and I am sure he could gips\\' clutching you for things. . . .\\\\\"\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0Slughorn both started and silently uncombanded with the little ears. Sut Ron was hungry and relax.Junsing blue, he had dropped his wand between his wand, which had gone where like a pair, his eyes wide, sitting rolling from his themble-toad\\'s Squibs. After a moment, the little knocker friendly we\\'ve never thought the Ministry were feeling her beigh - and o'\n",
            " b'name: when he found himself through the place, for the person changing of incredible: Dark idea what Harry had sustained a short way into a corner.\\\\\"Now, he left!\\\\\"\\xc2\\xa8Cmologion will not be refresing edilo, Apparently shabby ground and banging, last two yards, for her body slammed forward a shrivel, violet contact with loud trunks. Harry peered down at a lit that choped like a lump, even bewildered which Harry was keeping a harred black body with the goblin. \\\\\"And who still felt not much hisset; a wand\\'s second Grerway, then a funny, Tom,\\\\\" she said. \\\\\"Come in. I thought that there is, where I lived at the Castle. An old treasure that it odd whenever the ones who have part of the otherwied or someone\\'s hoping that the curse came zis pet all this upon is something? Because he did not wait there once \\xc2\\xa8C It was an odd cup yand was one last year, and for all of us, and what so uncertant they are face \\xc2\\xa8C \\\\\"\\\\\"What about the, the Ministry\\'s too?\\\\\" he said, \\\\\"but consider it will be permenting to cont'\n",
            " b'name: Dean, Slughorn recounted him where Mrs Potter four, the three of whom he saw, it has been another of the forest shall we a d - no large candles an old room, please, Slughorn has been produced, and I think - to deal was Weasley, who turned away at him as the look of a furshouse for him that Harry. Blimey closed with every word was washing inside it. He had been raised pubses line and start preventing a number player in space behind either. Under the side-quahed Keere back, rolled over. Ron spoke to listened, there was a long, pret-faced marringround the old swell... . . . He wanted him well, but what about what, Harry, thinks they knew Arthur was blasted away?\\\\\"\\\\\"No, I did, at last, apparently, you can\\'t imagine how things let me returning. And I was his own. All, you are still effected?\\\\\"Happine, idiths were beneated as successing goes more.\\\\\"What on either end this Patronus was still until that car it pointed his hood bubbles, but as Feeluncting Snape frozen cunior panting toward the'], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 5.773221492767334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Export the generator</h2>\n",
        "This single-step model can easily be saved and restored, allowing you to use it anywhere a tf.saved_model is accepted.\n"
      ],
      "metadata": {
        "id": "n3S6cepXj7WV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-YR2RLIVw5e",
        "outputId": "e5e3a031-a071-4355-ae2f-1bd10db1949c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f70e9592810>, because it is not built.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant(['name:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeMPutSdkC5z",
        "outputId": "5295ad9a-039b-494c-82de-074040c46638"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name: Firenze shouted as she gazed at the giants ¨C what's happened to his right?\\\"\\\"Very careful,\\\" said\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import io\n",
        "\n",
        "path = keras.utils.get_file(\n",
        "    \"nietzsche.txt\", origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\"\n",
        ")\n",
        "with io.open(path, encoding=\"utf-8\") as f:\n",
        "    text = f.read().lower()\n",
        "text = text.replace(\"\\n\", \" \")  # We remove newlines chars for nicer display\n",
        "print(\"Corpus length:\", len(text))\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "print(\"Total chars:\", len(chars))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "# cut the text in semi-redundant sequences of maxlen characters\n",
        "maxlen = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i : i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "print(\"Number of sequences:\", len(sentences))\n",
        "\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1\n",
        "\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(maxlen, len(chars))),\n",
        "        layers.LSTM(128),\n",
        "        layers.Dense(len(chars), activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "optimizer = keras.optimizers.RMSprop(learning_rate=0.01)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
        "\n",
        "\n",
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype(\"float64\")\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "epochs = 40\n",
        "batch_size = 128\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.fit(x, y, batch_size=batch_size, epochs=1)\n",
        "    print()\n",
        "    print(\"Generating text after epoch: %d\" % epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "        print(\"...Diversity:\", diversity)\n",
        "\n",
        "        generated = \"\"\n",
        "        sentence = text[start_index : start_index + maxlen]\n",
        "        print('...Generating with seed: \"' + sentence + '\"')\n",
        "\n",
        "        for i in range(400):\n",
        "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char_indices[char]] = 1.0\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = indices_char[next_index]\n",
        "            sentence = sentence[1:] + next_char\n",
        "            generated += next_char\n",
        "\n",
        "        print(\"...Generated: \", generated)\n",
        "        print()"
      ],
      "metadata": {
        "id": "bKqntIH9nXaQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32c61460-e3a7-42d0-eee3-1214e8afe843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
            "606208/600901 [==============================] - 0s 0us/step\n",
            "614400/600901 [==============================] - 0s 0us/step\n",
            "Corpus length: 600893\n",
            "Total chars: 56\n",
            "Number of sequences: 200285\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:32: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1565/1565 [==============================] - 31s 18ms/step - loss: 1.9270\n",
            "\n",
            "Generating text after epoch: 0\n",
            "...Diversity: 0.2\n",
            "...Generating with seed: \"in literature to their eyes and ears--th\"\n"
          ]
        }
      ]
    }
  ]
}