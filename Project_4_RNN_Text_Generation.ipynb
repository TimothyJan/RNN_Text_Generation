{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project 4-RNN-Text-Generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TimothyJan/RNN_Text_Generation/blob/main/Project_4_RNN_Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spring 2022 - CPSC 585-section-13883\n",
        "Project 4 - Recurrent Neural Networks Text Generation\n",
        "Sean Javiya\n",
        "Timothy Jan\n",
        "Timothy Kheang\n",
        "\n",
        "In Project 4, we use an RNN-based language model to generate text for a creative application.\n",
        "\n",
        "Goals for this project are:\n",
        "<ul>\n",
        "  <li>Reading about how RNNs can be used to generate text, and examining several different applications after training on different text corpora.</li>\n",
        "  <li>Using a multi-layer RNN to train and sample from a character-level language model.</li>\n",
        "  <li>Adapting and reusing published model code.</li>\n",
        "  <li>Having some fun.</li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "0JqoLNUQL6oU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCr_lbTxLs6e"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import urllib.request, json "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the Elder Scroll's or Shakespearean dataset and read the data."
      ],
      "metadata": {
        "id": "S4ctoJPB_paf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#'''imported the elder scrolls json file'''\n",
        "#with urllib.request.urlopen(\"https://raw.githubusercontent.com/hmi-utwente/video-game-text-corpora/master/The%20Elder%20Scrolls/data/imperial_library_20200626.json\") as url:\n",
        "#    data_dict = json.loads(url.read().decode())\n",
        "#text=\"\"\n",
        "#for key in data_dict:\n",
        "#  text += data_dict[key]['description'] +' ' + data_dict[key]['title'] + ' ' + data_dict[key]['text']\n",
        "#text[:100]\n",
        "\n",
        "#'''imported the aesops json file'''\n",
        "#with urllib.request.urlopen(\"https://raw.githubusercontent.com/itayniv/aesop-fables-stories/master/public/aesopFables.json\") as url:\n",
        "#    data_dict = json.loads(url.read().decode())\n",
        "#text=\"\"\n",
        "#for key, value in data_dict.items():\n",
        "#  for story in value:\n",
        "#    temp = dict(story)\n",
        "#    text += str(temp[\"story\"]) + \" \"\n",
        "#print(text[:100])\n",
        "\n",
        "# '''imported the Harry Potter text files'''\n",
        "text=\"\"\n",
        "with urllib.request.urlopen(\"https://raw.githubusercontent.com/ErikaJacobs/Harry-Potter-Text-Mining/master/Book%20Text/HPBook1.txt\") as url:\n",
        "    text += url.read().decode()\n",
        "with urllib.request.urlopen(\"https://raw.githubusercontent.com/ErikaJacobs/Harry-Potter-Text-Mining/master/Book%20Text/HPBook2.txt\") as url:\n",
        "    text += url.read().decode()\n",
        "with urllib.request.urlopen(\"https://raw.githubusercontent.com/ErikaJacobs/Harry-Potter-Text-Mining/master/Book%20Text/HPBook3.txt\") as url:\n",
        "    text += url.read().decode()\n",
        "with urllib.request.urlopen(\"https://raw.githubusercontent.com/ErikaJacobs/Harry-Potter-Text-Mining/master/Book%20Text/HPBook4.txt\") as url:\n",
        "    text += url.read().decode()\n",
        "with urllib.request.urlopen(\"https://raw.githubusercontent.com/ErikaJacobs/Harry-Potter-Text-Mining/master/Book%20Text/HPBook5.txt\") as url:\n",
        "    text += url.read().decode()\n",
        "with urllib.request.urlopen(\"https://raw.githubusercontent.com/ErikaJacobs/Harry-Potter-Text-Mining/master/Book%20Text/HPBook6.txt\") as url:\n",
        "    text += url.read().decode()\n",
        "with urllib.request.urlopen(\"https://raw.githubusercontent.com/ErikaJacobs/Harry-Potter-Text-Mining/master/Book%20Text/HPBook7.txt\") as url:\n",
        "    text += url.read().decode()\n",
        "\n",
        "# '''Shakespearen dataset'''\n",
        "# path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "# # Read, then decode for py2 compat.\n",
        "# text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# # length of text is the number of characters in it\n",
        "# print(f'Length of text: {len(text)} characters')\n",
        "# print(text[:100])"
      ],
      "metadata": {
        "id": "PjHAQwPv2ZQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the Data"
      ],
      "metadata": {
        "id": "hIQCraeN_v7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "id": "ODEUkCSo2uCf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e894fe2-2ec9-4fa6-a291-d1a3774c181e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "104 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Process the text</h1>\n",
        "<h2>Vectorize the text.</h2>\n",
        "Before training, you need to convert the strings to a numerical representation.\n",
        "The tf.keras.layers.StringLookup layer can convert each character into a numeric ID. It just needs the text to be split into tokens first."
      ],
      "metadata": {
        "id": "4c9STpH-_1qO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "id": "b6zWduFuuaIs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79506788-ac43-4740-dcf9-de6853e72dcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the tf.keras.layers.StringLookup layer. It onverts from tokens to character IDs:"
      ],
      "metadata": {
        "id": "wwsioWOgu7Pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)\n",
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "id": "ztbeaqT886Ni",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfbe259c-e55b-4999-8c7e-448d789955b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[66, 67, 68, 69, 70, 71, 72], [89, 90, 91]]>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This layer recovers the characters from the vectors of IDs, and returns them as a tf.RaggedTensor of characters:"
      ],
      "metadata": {
        "id": "YMRHxp5gvSsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "id": "fHnRKiJZuEN_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "797ad0f6-0de8-496c-ea79-2b2f21587c6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use tf.strings.reduce_join to join the characters back into strings."
      ],
      "metadata": {
        "id": "fQsbjGoQvlZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "wU3pOFakvUMx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e73145e8-247e-4193-986a-53845ebc01d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n"
      ],
      "metadata": {
        "id": "jQJmm671vqst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>The Prediction Task</h2>\n",
        "Given a character, or a sequence of characters, what is the most probable next character? This is the task you're training the model to perform. The input to the model will be a sequence of characters, and you train the model to predict the output—the following character at each time step.\n",
        "\n",
        "Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?\n",
        "\n",
        "<h2>Create training examples and targets</h2>\n",
        "Next divide the text into example sequences. Each input sequence will contain seq_length characters from the text.\n",
        "\n",
        "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
        "\n",
        "So break the text into chunks of seq_length+1. For example, say seq_length is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
        "\n",
        "To do this first use the tf.data.Dataset.from_tensor_slices function to convert the text vector into a stream of character indices."
      ],
      "metadata": {
        "id": "ygvRQ68jwFGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "id": "n07gqUwPvtCD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0be13852-1712-4837-cb9f-063b943de411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(6325064,), dtype=int64, numpy=array([ 5, 53, 70, ..., 33, 24,  2])>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wywrnl1_wj4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "id": "iWdmxTW4wUQ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22173153-2963-4fc0-a1f8-b5e87e2fed2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"\n",
            "T\n",
            "e\n",
            "x\n",
            "t\n",
            "\"\n",
            "@\n",
            "\"\n",
            "C\n",
            "h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The batch method lets you easily convert these individual characters to sequences of the desired size."
      ],
      "metadata": {
        "id": "kqaTSg_kwsg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "id": "zsYC0q3OwhPk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e521b9c6-3dab-4c14-d847-4e3a9dec06d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'\"' b'T' b'e' b'x' b't' b'\"' b'@' b'\"' b'C' b'h' b'a' b'p' b't' b'e'\n",
            " b'r' b'\"' b'@' b'\"' b'B' b'o' b'o' b'k' b'\"' b'\\n' b'\"' b'T' b'H' b'E'\n",
            " b' ' b'B' b'O' b'Y' b' ' b'W' b'H' b'O' b' ' b'L' b'I' b'V' b'E' b'D'\n",
            " b' ' b' ' b'M' b'r' b'.' b' ' b'a' b'n' b'd' b' ' b'M' b'r' b's' b'.'\n",
            " b' ' b'D' b'u' b'r' b's' b'l' b'e' b'y' b',' b' ' b'o' b'f' b' ' b'n'\n",
            " b'u' b'm' b'b' b'e' b'r' b' ' b'f' b'o' b'u' b'r' b',' b' ' b'P' b'r'\n",
            " b'i' b'v' b'e' b't' b' ' b'D' b'r' b'i' b'v' b'e' b',' b' ' b'w' b'e'\n",
            " b'r' b'e' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "id": "erVoYFeVwvTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02cc9f66-d64a-4ffd-b342-61afcbd10f90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'\"Text\"@\"Chapter\"@\"Book\"\\n\"THE BOY WHO LIVED  Mr. and Mrs. Dursley, of number four, Privet Drive, were '\n",
            "b\"proud to say that they were perfectly normal, thank you very much. They were the last people you'd ex\"\n",
            "b\"pect to be involved in anything strange or mysterious, because they just didn't hold with such nonsen\"\n",
            "b'se.  Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy '\n",
            "b'man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blond'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For training you'll need a dataset of (input, label) pairs. Where input and label are sequences. At each time step the input is the current character and the label is the next character.\n",
        "\n",
        "Here's a function that takes a sequence as input, duplicates, and shifts it to align the input and label for each timestep:"
      ],
      "metadata": {
        "id": "OwKzugSsxLYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "split_input_target(list(\"Tensorflow\"))\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "id": "ng09QHSGwyCB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94b9eaef-f764-498a-cfe2-ba9d38ba8b73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'\"Text\"@\"Chapter\"@\"Book\"\\n\"THE BOY WHO LIVED  Mr. and Mrs. Dursley, of number four, Privet Drive, were'\n",
            "Target: b'Text\"@\"Chapter\"@\"Book\"\\n\"THE BOY WHO LIVED  Mr. and Mrs. Dursley, of number four, Privet Drive, were '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Create training batches</h2>\n",
        "You used tf.data to split the text into manageable sequences. But before feeding this data into the model, you need to shuffle the data and pack it into batches."
      ],
      "metadata": {
        "id": "uWNNN1wYxfuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "Lg5_JLblxWJs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "688dabd1-8169-4e2b-92d0-dd4e0e8df000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Build The Model</h1>\n",
        "This section defines the model as a keras.Model subclass (For details see Making new Layers and Models via subclassing).\n",
        "\n",
        "This model has three layers:\n",
        "<ul>\n",
        "  <li>tf.keras.layers.Embedding: The input layer. A trainable lookup table that will map each character-ID to a vector with embedding_dim dimensions;</li>\n",
        "  <li>tf.keras.layers.GRU: A type of RNN with size units=rnn_units (You can also use an LSTM layer here.)</li>\n",
        "  <li>tf.keras.layers.Dense: The output layer, with vocab_size outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model.</li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "XWvJPLPIx1p0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x\n",
        "\n",
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "kpOtbBZ5xp9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Try the model</h1>\n",
        "Now run the model to see that it behaves as expected.\n",
        "\n",
        "First check the shape of the output:"
      ],
      "metadata": {
        "id": "6MBXoYiGyN1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "id": "IlMFJ1l2yL6N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6a215fd-5ad9-424b-ee96-1de70fcd4797"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 105) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "Mm89Pc-1Ut1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5f12518-8953-488d-d798-e83b0e1367b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  26880     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  107625    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,072,809\n",
            "Trainable params: 4,072,809\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get actual predictions from the model you need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
        "using the first example in the batch, this gives us, at each timestep, a prediction of the next character index:"
      ],
      "metadata": {
        "id": "QR9a-yGJU-ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "sampled_indices"
      ],
      "metadata": {
        "id": "ZkggEVwZU5wY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b6804fb-6afa-4fa7-af38-3ca78860d1eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 67,  48,  29,  86,  18,   5,  26,  84,  40,  77,  10,   5,  69,\n",
              "        58,  60,  70,  90,  67,  96,  60,  23,  39,  58,  33,  31,  27,\n",
              "        55,  65,  49,  52,  67,  75,  38,  97,  97,  25,  72,   7,  48,\n",
              "        31,  96,  68,  59,  96,  22,  95,  86,  61,  49,  65,   2,  93,\n",
              "        66, 101,  25,  98, 102,  76,  20,  19,  80,  16,   0,   7,  45,\n",
              "        66,  39,  95,  61,  18,  85,  60, 103,  43,  21,  70,  80,  90,\n",
              "        89,  22,  30,  44,  49,  85,   0,  57,   5,  71,  58,  40,  35,\n",
              "        73,  96,  34,  80,  80,  94,   9,  97,  64])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoded text predicted by untrained model:\n"
      ],
      "metadata": {
        "id": "qXTNGBVnVSUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "id": "wg3ryi3fVGda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00ad7caf-4467-49de-c482-aeb592951848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'arry\\'s legs began to jerk around out of his control in a kind of quickstep.  \\\\\"Stop! Stop!\\\\\" screame'\n",
            "\n",
            "Next Char Predictions:\n",
            " b'bO<u1\"9sGl(\"dY[eyb\\xc2\\xa6[6FY@>:V`PSbjE\\xc2\\xa8\\xc2\\xa88g%O>\\xc2\\xa6cZ\\xc2\\xa65\\xc2\\xa0u\\\\P`\\n}a\\xe2\\x80\\x998\\xe2\\x80\\x93\\xe2\\x80\\x9ck32o/[UNK]%LaF\\xc2\\xa0\\\\1t[\\xe2\\x80\\x9dJ4eoyx5=KPt[UNK]X\"fYGBh\\xc2\\xa6Aoo~\\'\\xc2\\xa8_'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Train the model</h1>\n",
        "Given the previous RNN state, and the input this time step, predict the class of the next character.\n",
        "<h2>Attach an optimizer, and a loss function</h2>\n",
        "The standard tf.keras.losses.sparse_categorical_crossentropy loss function works in this case because it is applied across the last dimension of the predictions.\n",
        "\n",
        "Because our model returns logits, you need to set the from_logits flag."
      ],
      "metadata": {
        "id": "jUX43TNRVdFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "id": "YdjtU7qfVWbZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "907b86d9-10b9-49b6-cb0e-c8e463c0850f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 105)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.6527653, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A newly initialized model shouldn't be too sure of itself, the output logits should all have similar magnitudes. To confirm this you can check that the exponential of the mean loss is approximately equal to the vocabulary size. A much higher loss means the model is sure of its wrong answers, and is badly initialized:"
      ],
      "metadata": {
        "id": "qgRLi-RlV1pL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "id": "qEPkvmWDVuxh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74cdedec-f253-46fe-cbb3-9b19aab20d7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "104.874596"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure the training procedure using the tf.keras.Model.compile method. Use tf.keras.optimizers.Adam with default arguments and the loss function."
      ],
      "metadata": {
        "id": "s73s12hhV4-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "gQOtT1qiV3TC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Configure Checkpoints</h2>\n",
        "Use a tf.keras.callbacks.ModelCheckpoint to ensure that checkpoints are saved during training:"
      ],
      "metadata": {
        "id": "zo1KAh-EV9ca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "lopLf__XV6i9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Execute the training</h2>\n",
        "To keep training time reasonable, use 10 epochs to train the model. In Colab, set the runtime to GPU for faster training."
      ],
      "metadata": {
        "id": "J6pgF9vsWPtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#EPOCHS = 50\n",
        "EPOCHS = 20\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aJ9q8u_WDxD",
        "outputId": "8f8fd8c3-cf16-40a3-b102-530c60eedb18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "978/978 [==============================] - 134s 129ms/step - loss: 1.7618\n",
            "Epoch 2/20\n",
            "345/978 [=========>....................] - ETA: 1:22 - loss: 1.2532"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Generate text</h1>\n",
        "The simplest way to generate text with this model is to run it in a loop, and keep track of the model's internal state as you execute it.<br>\n",
        "Each time you call the model you pass in some text and an internal state. The model returns a prediction for the next character and its new state. Pass the prediction and state back in to continue generating text.\n",
        "\n",
        "The following makes a single step prediction:"
      ],
      "metadata": {
        "id": "J8AxIH-pVOZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "seRFW5wzWOyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "XPixnmNaVl3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run it in a loop to generate some text. Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
      ],
      "metadata": {
        "id": "MiLk6_irVZwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['Character_Name:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "id": "-WcC5oMbVYJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The easiest thing you can do to improve the results is to train it for longer (try EPOCHS = 30).\n",
        "\n",
        "You can also experiment with a different start string, try adding another RNN layer to improve the model's accuracy, or adjust the temperature parameter to generate more or less random predictions.\n",
        "\n",
        "If you want the model to generate text faster the easiest thing you can do is batch the text generation. In the example below the model generates 5 outputs in about the same time it took to generate 1 above."
      ],
      "metadata": {
        "id": "wONo0ImkVtId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['name:', 'name:', 'name:', 'name:', 'name:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "id": "ijNGYT8_VhAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Export the generator</h2>\n",
        "This single-step model can easily be saved and restored, allowing you to use it anywhere a tf.saved_model is accepted.\n"
      ],
      "metadata": {
        "id": "n3S6cepXj7WV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "id": "k-YR2RLIVw5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant(['name:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "id": "VeMPutSdkC5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import io\n",
        "\n",
        "path = keras.utils.get_file(\n",
        "    \"nietzsche.txt\", origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\"\n",
        ")\n",
        "with io.open(path, encoding=\"utf-8\") as f:\n",
        "    text = f.read().lower()\n",
        "text = text.replace(\"\\n\", \" \")  # We remove newlines chars for nicer display\n",
        "print(\"Corpus length:\", len(text))\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "print(\"Total chars:\", len(chars))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "# cut the text in semi-redundant sequences of maxlen characters\n",
        "maxlen = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i : i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "print(\"Number of sequences:\", len(sentences))\n",
        "\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1\n",
        "\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(maxlen, len(chars))),\n",
        "        layers.LSTM(128),\n",
        "        layers.Dense(len(chars), activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "optimizer = keras.optimizers.RMSprop(learning_rate=0.01)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
        "\n",
        "\n",
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype(\"float64\")\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "epochs = 40\n",
        "batch_size = 128\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.fit(x, y, batch_size=batch_size, epochs=1)\n",
        "    print()\n",
        "    print(\"Generating text after epoch: %d\" % epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "        print(\"...Diversity:\", diversity)\n",
        "\n",
        "        generated = \"\"\n",
        "        sentence = text[start_index : start_index + maxlen]\n",
        "        print('...Generating with seed: \"' + sentence + '\"')\n",
        "\n",
        "        for i in range(400):\n",
        "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char_indices[char]] = 1.0\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = indices_char[next_index]\n",
        "            sentence = sentence[1:] + next_char\n",
        "            generated += next_char\n",
        "\n",
        "        print(\"...Generated: \", generated)\n",
        "        print()"
      ],
      "metadata": {
        "id": "bKqntIH9nXaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YBMqbW5CNmrC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}